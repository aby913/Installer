
 _____                    _                 
/__   \___ _ __ _ __ ___ (_)_ __  _   _ ___ 
  / /\/ _ \ '__| '_ ` _ \| | '_ \| | | / __|
 / / |  __/ |  | | | | | | | | | | |_| \__ \
 \/   \___|_|  |_| |_| |_|_|_| |_|\__,_|___/
                                            

2024-07-09T02:05:29.106-0700	current user: root
2024-07-09T02:05:29.109-0700	[Job] [Startup] start ...
2024-07-09T02:05:29.109-0700	[Module] GetMachineInfo
2024-07-09T02:05:29.109-0700	[A] LocalHost: GetMachineInfo
2024-07-09T02:05:29.112-0700	[A] LocalHost: GetMachineInfo success (2.808403ms)
2024-07-09T02:05:29.112-0700	[A] LocalHost: GetCgroupsEnabled
MACHINE, hostname: zhaoyu, cpu: 14, mem: 30.98 GB, disk: 937.33 GB, local-ip: 
SYSTEM, os: linux, platform: ubuntu, arch: amd64, version: 22.04
CGROUP, cpu-enabled: 1, memory-enabled: 1
2024-07-09T02:05:29.112-0700	[A] LocalHost: GetCgroupsEnabled success (41.028µs)
2024-07-09T02:05:29.112-0700	[A] LocalHost: GetLocalIp
2024-07-09T02:05:29.112-0700	[A] LocalHost: GetLocalIp success (115.182µs)
2024-07-09T02:05:29.112-0700	[Module] CopyScripts
2024-07-09T02:05:29.112-0700	[A] LocalHost: CopyScripts
2024-07-09T02:05:29.113-0700	[A] LocalHost: CopyScripts success (131.386µs)
2024-07-09T02:05:29.113-0700	[A] LocalHost: Greeting
2024-07-09T02:05:29.125-0700	script CMD: /project/scripts/greeting.sh, OUTPUT: 
ubuntu
test script greetings
lsb_dist=ubuntu
BASE_DIR=/project/scripts
2024-07-09T02:05:29.125-0700	[A] LocalHost: Greeting success (12.536293ms)
2024-07-09T02:05:29.125-0700	[Job] Startup execute successfully!!! (15.719382ms)
2024-07-09T02:05:41.644-0700	[Job] [Install Terminus] start ...
2024-07-09T02:05:41.645-0700	[Module] TerminusGreeting
2024-07-09T02:05:41.645-0700	[A] LocalHost: Greetings
2024-07-09T02:05:41.645-0700	[A] LocalHost: Greetings success (514.938µs)
2024-07-09T02:05:41.645-0700	[Module] SaveInstallConfig
2024-07-09T02:05:41.645-0700	[A] LocalHost: Save
2024-07-09T02:05:41.654-0700	[A] LocalHost: Save success (8.403382ms)
2024-07-09T02:05:41.654-0700	[Module] PreloadImages
2024-07-09T02:05:41.654-0700	[A] LocalHost: PreloadK3sImages
2024-07-09T02:05:41.755-0700	[A] LocalHost: PreloadK3sImages success (100.94841ms)
2024-07-09T02:05:41.755-0700	[Module] GreetingsModule
2024-07-09T02:05:41.996-0700	[A] Remote: Greetings
2024-07-09T02:05:42.010-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "echo 'Greetings, KubeKey!!!!! hahahaha!!!!'", OUTPUT: 
Greetings, KubeKey!!!!! hahahaha!!!!
2024-07-09T02:05:42.010-0700	[A] zhaoyu: Greetings success (255.620404ms)
2024-07-09T02:05:42.010-0700	[Module] K3sNodeBinariesModule
2024-07-09T02:05:42.010-0700	[A] LocalHost: DownloadBinaries
---helm zone / 1--- 
---helm url / 2--- https://get.helm.sh/helm-v3.9.0-linux-amd64.tar.gz
2024-07-09T02:05:42.954-0700	get file helm size: 13.31 MB
---untarCmd / 1--- cd /project/pkg/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64*
---untarCmd / 5--- exit status 2
2024-07-09T02:05:48.344-0700	decompression failed: exit status 2
2024-07-09T02:05:49.611-0700	get file helm size: 13.31 MB
---untarCmd / 1--- cd /project/pkg/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64*
---untarCmd / 5--- exit status 2
2024-07-09T02:05:50.437-0700	decompression failed: exit status 2
2024-07-09T02:05:51.762-0700	get file helm size: 13.31 MB
---untarCmd / 1--- cd /project/pkg/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64*
---untarCmd / 5--- exit status 2
2024-07-09T02:05:52.901-0700	decompression failed: exit status 2
2024-07-09T02:05:54.118-0700	get file helm size: 13.31 MB
---untarCmd / 1--- cd /project/pkg/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64*
---untarCmd / 5--- exit status 2
2024-07-09T02:05:54.945-0700	decompression failed: exit status 2
2024-07-09T02:05:56.163-0700	get file helm size: 13.31 MB
---untarCmd / 1--- cd /project/pkg/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64*
---untarCmd / 5--- exit status 2
2024-07-09T02:05:56.992-0700	decompression failed: exit status 2
2024-07-09T02:05:58.144-0700	[A] LocalHost: DownloadBinaries success (16.133957876s)
2024-07-09T02:05:58.144-0700	[Module] ConfigureOSModule
2024-07-09T02:05:58.144-0700	[A] Remote: GetOSData
---1--- PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
2024-07-09T02:05:58.155-0700	[A] zhaoyu: GetOSData success (10.2238ms)
2024-07-09T02:05:58.155-0700	[A] Remote: InitOS
2024-07-09T02:05:58.404-0700	[A] zhaoyu: InitOS success (249.459081ms)
2024-07-09T02:05:58.404-0700	[A] Remote: GenerateScript
2024-07-09T02:05:58.424-0700	scp local file /project/pkg/zhaoyu/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
2024-07-09T02:05:58.448-0700	[A] zhaoyu: GenerateScript success (44.010919ms)
2024-07-09T02:05:58.448-0700	[A] Remote: ExecScript
2024-07-09T02:05:59.350-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh", OUTPUT: 
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 1
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_tw_buckets = 32768
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_keepalive_time = 1800
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_fin_timeout = 10
vm.overcommit_memory = 1
net.core.somaxconn = 10240
2024-07-09T02:05:59.350-0700	[A] zhaoyu: ExecScript success (901.784949ms)
2024-07-09T02:05:59.350-0700	[A] zhaoyu: ConfigureNtpServer skipped (33.332µs)
2024-07-09T02:05:59.350-0700	[Module] StatusModule
2024-07-09T02:05:59.350-0700	[A] Remote: GetClusterStatus(k3s)
2024-07-09T02:05:59.373-0700	check remote file exist: false
2024-07-09T02:05:59.373-0700	[A] zhaoyu: GetClusterStatus(k3s) success (23.241163ms)
2024-07-09T02:05:59.373-0700	[Module] ETCDPreCheckModule
2024-07-09T02:05:59.373-0700	[A] Remote: GetETCDStatus
2024-07-09T02:05:59.382-0700	check remote file exist: false
2024-07-09T02:05:59.382-0700	[A] zhaoyu: GetETCDStatus success (8.493172ms)
2024-07-09T02:05:59.382-0700	[Module] CertsModule
2024-07-09T02:05:59.382-0700	[A] Remote: FetchETCDCerts
2024-07-09T02:05:59.382-0700	[A] zhaoyu: FetchETCDCerts success (29.999µs)
2024-07-09T02:05:59.382-0700	[A] LocalHost: GenerateETCDCerts
[certs] Generating "ca" certificate and key
[certs] admin-zhaoyu serving cert is signed for DNS names [etcd etcd.kube-system etcd.kube-system.svc etcd.kube-system.svc.cluster.local lb.kubesphere.local localhost zhaoyu] and IPs [127.0.0.1 ::1 192.168.50.187]
[certs] member-zhaoyu serving cert is signed for DNS names [etcd etcd.kube-system etcd.kube-system.svc etcd.kube-system.svc.cluster.local lb.kubesphere.local localhost zhaoyu] and IPs [127.0.0.1 ::1 192.168.50.187]
[certs] node-zhaoyu serving cert is signed for DNS names [etcd etcd.kube-system etcd.kube-system.svc etcd.kube-system.svc.cluster.local lb.kubesphere.local localhost zhaoyu] and IPs [127.0.0.1 ::1 192.168.50.187]
2024-07-09T02:05:59.608-0700	[A] LocalHost: GenerateETCDCerts success (225.573812ms)
2024-07-09T02:05:59.608-0700	[A] Remote: SyncCertsFile
2024-07-09T02:05:59.635-0700	scp local file /project/pkg/pki/etcd/ca.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/ca.pem success
2024-07-09T02:05:59.680-0700	scp local file /project/pkg/pki/etcd/ca-key.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/ca-key.pem success
2024-07-09T02:05:59.720-0700	scp local file /project/pkg/pki/etcd/admin-zhaoyu.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/admin-zhaoyu.pem success
2024-07-09T02:05:59.760-0700	scp local file /project/pkg/pki/etcd/admin-zhaoyu-key.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/admin-zhaoyu-key.pem success
2024-07-09T02:05:59.799-0700	scp local file /project/pkg/pki/etcd/member-zhaoyu.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/member-zhaoyu.pem success
2024-07-09T02:05:59.839-0700	scp local file /project/pkg/pki/etcd/member-zhaoyu-key.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/member-zhaoyu-key.pem success
2024-07-09T02:05:59.880-0700	scp local file /project/pkg/pki/etcd/node-zhaoyu.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/node-zhaoyu.pem success
2024-07-09T02:05:59.918-0700	scp local file /project/pkg/pki/etcd/node-zhaoyu-key.pem to remote /tmp/kubekey/etc/ssl/etcd/ssl/node-zhaoyu-key.pem success
2024-07-09T02:05:59.940-0700	[A] zhaoyu: SyncCertsFile success (332.632167ms)
2024-07-09T02:05:59.940-0700	[A] zhaoyu: SyncCertsFileToMaster skipped (21.552µs)
2024-07-09T02:05:59.940-0700	[Module] InstallETCDBinaryModule
2024-07-09T02:05:59.940-0700	[A] Remote: InstallETCDBinary
2024-07-09T02:06:00.069-0700	scp local file /project/pkg/etcd/v3.4.13/amd64/etcd-v3.4.13-linux-amd64.tar.gz to remote /tmp/kubekey/etcd-v3.4.13-linux-amd64.tar.gz success
2024-07-09T02:06:00.383-0700	[A] zhaoyu: InstallETCDBinary success (442.535708ms)
2024-07-09T02:06:00.383-0700	[A] Remote: GenerateETCDService
2024-07-09T02:06:00.415-0700	scp local file /project/pkg/zhaoyu/etcd.service to remote /tmp/kubekey/etc/systemd/system/etcd.service success
2024-07-09T02:06:00.441-0700	[A] zhaoyu: GenerateETCDService success (58.419636ms)
2024-07-09T02:06:00.441-0700	[A] Remote: GenerateAccessAddress
2024-07-09T02:06:00.441-0700	[A] zhaoyu: GenerateAccessAddress success (26.488µs)
2024-07-09T02:06:00.441-0700	[Module] ETCDConfigureModule
2024-07-09T02:06:00.441-0700	[A] zhaoyu: ExistETCDHealthCheck skipped (8.054µs)
2024-07-09T02:06:00.441-0700	[A] Remote: GenerateETCDConfig
2024-07-09T02:06:00.460-0700	scp local file /project/pkg/zhaoyu/etcd.env to remote /tmp/kubekey/etc/etcd.env success
2024-07-09T02:06:00.482-0700	[A] zhaoyu: GenerateETCDConfig success (40.436148ms)
2024-07-09T02:06:00.482-0700	[A] Remote: AllRefreshETCDConfig
2024-07-09T02:06:00.501-0700	scp local file /project/pkg/zhaoyu/etcd.env to remote /tmp/kubekey/etc/etcd.env success
2024-07-09T02:06:00.522-0700	[A] zhaoyu: AllRefreshETCDConfig success (40.531212ms)
2024-07-09T02:06:00.522-0700	[A] Remote: RestartETCD
2024-07-09T02:06:03.193-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "systemctl daemon-reload && systemctl restart etcd && systemctl enable etcd", OUTPUT: 

2024-07-09T02:06:03.193-0700	[A] zhaoyu: RestartETCD success (2.670123416s)
2024-07-09T02:06:03.193-0700	[A] Remote: AllETCDNodeHealthCheck
2024-07-09T02:06:03.218-0700	[A] zhaoyu: AllETCDNodeHealthCheck success (25.19503ms)
2024-07-09T02:06:03.218-0700	[A] Remote: RefreshETCDConfigToExist
2024-07-09T02:06:03.236-0700	scp local file /project/pkg/zhaoyu/etcd.env to remote /tmp/kubekey/etc/etcd.env success
2024-07-09T02:06:03.259-0700	[A] zhaoyu: RefreshETCDConfigToExist success (41.406093ms)
2024-07-09T02:06:03.259-0700	[A] Remote: AllETCDNodeHealthCheck
2024-07-09T02:06:03.279-0700	[A] zhaoyu: AllETCDNodeHealthCheck success (19.569502ms)
2024-07-09T02:06:03.279-0700	[Module] ETCDBackupModule
2024-07-09T02:06:03.279-0700	[A] Remote: BackupETCD
2024-07-09T02:06:03.297-0700	scp local file /project/pkg/zhaoyu/etcd-backup.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/etcd-backup.sh success
2024-07-09T02:06:03.325-0700	[A] zhaoyu: BackupETCD success (45.851348ms)
2024-07-09T02:06:03.325-0700	[A] Remote: GenerateBackupETCDService
2024-07-09T02:06:03.343-0700	scp local file /project/pkg/zhaoyu/backup-etcd.service to remote /tmp/kubekey/etc/systemd/system/backup-etcd.service success
2024-07-09T02:06:03.365-0700	[A] zhaoyu: GenerateBackupETCDService success (39.732673ms)
2024-07-09T02:06:03.365-0700	[A] Remote: GenerateBackupETCDTimer
2024-07-09T02:06:03.383-0700	scp local file /project/pkg/zhaoyu/backup-etcd.timer to remote /tmp/kubekey/etc/systemd/system/backup-etcd.timer success
2024-07-09T02:06:03.405-0700	[A] zhaoyu: GenerateBackupETCDTimer success (40.503347ms)
2024-07-09T02:06:03.405-0700	[A] Remote: EnableBackupETCDService
2024-07-09T02:06:03.561-0700	[A] zhaoyu: EnableBackupETCDService success (155.574482ms)
2024-07-09T02:06:03.561-0700	[Module] InstallKubeBinariesModule
2024-07-09T02:06:03.561-0700	[A] Remote: SyncKubeBinary(k3s)
2024-07-09T02:06:03.570-0700	SyncKubeBinary cp k3s from /project/pkg/kube/v1.22.16/amd64/k3s to /usr/local/bin/k3s

2024-07-09T02:06:03.877-0700	scp local file /project/pkg/kube/v1.22.16/amd64/k3s to remote /tmp/kubekey/usr/local/bin/k3s success
2024-07-09T02:06:03.910-0700	SyncKubeBinary cp helm from /project/pkg/helm/v3.9.0/amd64/helm to /usr/local/bin/helm

2024-07-09T02:06:04.003-0700	scp local file /project/pkg/helm/v3.9.0/amd64/helm to remote /tmp/kubekey/usr/local/bin/helm success
2024-07-09T02:06:04.039-0700	SyncKubeBinary cp kubecni from /project/pkg/cni/v0.9.1/amd64/cni-plugins-linux-amd64-v0.9.1.tgz to /tmp/kubekey/cni-plugins-linux-amd64-v0.9.1.tgz

2024-07-09T02:06:04.268-0700	scp local file /project/pkg/cni/v0.9.1/amd64/cni-plugins-linux-amd64-v0.9.1.tgz to remote /tmp/kubekey/cni-plugins-linux-amd64-v0.9.1.tgz success
2024-07-09T02:06:04.728-0700	[A] zhaoyu: SyncKubeBinary(k3s) success (1.166927012s)
2024-07-09T02:06:04.728-0700	[A] Remote: GenerateK3sKillAllScript
2024-07-09T02:06:04.747-0700	scp local file /project/pkg/zhaoyu/k3s-killall.sh to remote /tmp/kubekey/usr/local/bin/k3s-killall.sh success
2024-07-09T02:06:04.772-0700	[A] zhaoyu: GenerateK3sKillAllScript success (44.241923ms)
2024-07-09T02:06:04.772-0700	[A] Remote: GenerateK3sUninstallScript
2024-07-09T02:06:04.791-0700	scp local file /project/pkg/zhaoyu/k3s-uninstall.sh to remote /tmp/kubekey/usr/local/bin/k3s-uninstall.sh success
2024-07-09T02:06:04.812-0700	[A] zhaoyu: GenerateK3sUninstallScript success (40.42049ms)
2024-07-09T02:06:04.812-0700	[A] Remote: ChmodScript(k3s)
2024-07-09T02:06:04.826-0700	[A] zhaoyu: ChmodScript(k3s) success (13.830527ms)
2024-07-09T02:06:04.826-0700	[Module] K3sInitClusterModule
2024-07-09T02:06:04.826-0700	[A] Remote: GenerateK3sService
2024-07-09T02:06:04.827-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:06:04.846-0700	scp local file /project/pkg/zhaoyu/k3s.service to remote /tmp/kubekey/etc/systemd/system/k3s.service success
2024-07-09T02:06:04.886-0700	scp local file /project/pkg/zhaoyu/kubelet.config to remote /tmp/kubekey/etc/rancher/k3s/kubelet.config success
2024-07-09T02:06:04.912-0700	[A] zhaoyu: GenerateK3sService success (85.859237ms)
2024-07-09T02:06:04.912-0700	[A] Remote: GenerateK3sServiceEnv
2024-07-09T02:06:04.930-0700	scp local file /project/pkg/zhaoyu/k3s.service.env to remote /tmp/kubekey/etc/systemd/system/k3s.service.env success
2024-07-09T02:06:04.952-0700	[A] zhaoyu: GenerateK3sServiceEnv success (39.588565ms)
2024-07-09T02:06:04.952-0700	[A] Remote: GenerateK3sRegistryConfig
2024-07-09T02:06:04.971-0700	scp local file /project/pkg/zhaoyu/registries.yaml to remote /tmp/kubekey/etc/rancher/k3s/registries.yaml success
2024-07-09T02:06:04.993-0700	[A] zhaoyu: GenerateK3sRegistryConfig success (41.535787ms)
2024-07-09T02:06:04.993-0700	[A] Remote: EnableK3sService
2024-07-09T02:06:09.741-0700	[A] zhaoyu: EnableK3sService success (4.748170551s)
2024-07-09T02:06:09.742-0700	[A] Remote: PreloadImagesService
ctr -n k8s.io images import - load image /var/lib/images/00a918727d85acaa2c46b00c5a058d08.tar.gz success in 1.943990874s
ctr -n k8s.io images import - load image /var/lib/images/018a8a54f11306dd8baa11ef228f8fa6.tar.gz success in 1.399981073s
ctr -n k8s.io images import - load image /var/lib/images/01f8906de38078d641c8b287ebc5dd5f.tar.gz success in 2.495371556s
ctr -n k8s.io images import - load image /var/lib/images/0b50ecb60c023f1b7e881c77fb46e0d7.tar.gz success in 1.652852859s
ctr -n k8s.io images import - load image /var/lib/images/10eff9a8c6dab1a3475e0c79c7d28e4e.tar.gz success in 1.559667016s
ctr -n k8s.io images import - load image /var/lib/images/127c0408d69702361838c44a8e2767d9.tar.gz success in 264.469738ms
ctr -n k8s.io images import - load image /var/lib/images/168476169397016d88acdb1d638d1b4a.tar.gz success in 3.036574269s
ctr -n k8s.io images import - load image /var/lib/images/16b9730ef93737d2c30782c6ccb41f8c.tar.gz success in 986.755201ms
ctr -n k8s.io images import - load image /var/lib/images/182d4644e9219113c564d18bf469d538.tar.gz success in 1.980234175s
ctr -n k8s.io images import - load image /var/lib/images/19ddd92a3d93190a61b3d097889c2da3.tar.gz success in 1.40679953s
ctr -n k8s.io images import - load image /var/lib/images/1c6eb53969976ae6e4c40032c9d0ceef.tar.gz success in 7.36279873s
ctr -n k8s.io images import - load image /var/lib/images/1fba1bf70d364fa06aa14cea3785115c.tar.gz success in 3.154842447s
ctr -n k8s.io images import - load image /var/lib/images/2068fb7232130a241a89861518a776f9.tar.gz success in 1.451518143s
ctr -n k8s.io images import - load image /var/lib/images/20b241492dca842e69071c9c2ab67f50.tar.gz success in 3.059563274s
ctr -n k8s.io images import - load image /var/lib/images/20b721e909f602c53a1e3867f5136450.tar.gz success in 3.408311766s
ctr -n k8s.io images import - load image /var/lib/images/2176e21fd002dd62e8b64d70814f5f62.tar.gz success in 1.663797333s
ctr -n k8s.io images import - load image /var/lib/images/23aea631f2a00fa6259e40385b242669.tar.gz success in 1.059350986s
ctr -n k8s.io images import - load image /var/lib/images/247da07dd4f25e0d1a319ef7a09dd5b7.tar.gz success in 2.633937213s
ctr -n k8s.io images import - load image /var/lib/images/24e6a95cf36193a9de0dfe3b07d2fc1f.tar.gz success in 1.421451362s
ctr -n k8s.io images import - load image /var/lib/images/2bd8ca01d96302470780fcdd26bfac5b.tar.gz success in 1.369848691s
ctr -n k8s.io images import - load image /var/lib/images/2e4cfaf9d856ad5ffe0813873d491575.tar.gz success in 1.835589951s
ctr -n k8s.io images import - load image /var/lib/images/31df8a1f11d4dc1dfdf3673690442b19.tar.gz success in 1.269166219s
ctr -n k8s.io images import - load image /var/lib/images/352bea84f3c47a1a36242798190782b4.tar.gz success in 1.104832771s
ctr -n k8s.io images import - load image /var/lib/images/35f8eb2789f840c13a0e993bbb240600.tar.gz success in 1.732124956s
ctr -n k8s.io images import - load image /var/lib/images/42bb09c63a5880640177fda29a0c395c.tar.gz success in 1.342770822s
ctr -n k8s.io images import - load image /var/lib/images/45530e261409d2fcd651b54badd46d09.tar.gz success in 1.67976404s
ctr -n k8s.io images import - load image /var/lib/images/483fa485dac7b88a919f707488fade6e.tar.gz success in 487.174051ms
ctr -n k8s.io images import - load image /var/lib/images/4c8a6ff8986af83a13abf71ce21ff628.tar.gz success in 2.371319199s
ctr -n k8s.io images import - load image /var/lib/images/4f6695cbd6660cb56aabc4bbf9abb230.tar.gz success in 1.439606945s
ctr -n k8s.io images import - load image /var/lib/images/4fe1db618b586b22b9c1147a60141840.tar.gz success in 1.186623112s
ctr -n k8s.io images import - load image /var/lib/images/4ff264a20fa8681584e4b638624ea776.tar.gz success in 3.968632457s
ctr -n k8s.io images import - load image /var/lib/images/521564c4b60ae73c78899b7b40ae655e.tar.gz success in 2.404490589s
ctr -n k8s.io images import - load image /var/lib/images/521769f3eb1dc383e4a1b5176043fef9.tar.gz success in 3.2975451s
ctr -n k8s.io images import - load image /var/lib/images/523e19e91147898f65700c325d0b143e.tar.gz success in 2.332177s
ctr -n k8s.io images import - load image /var/lib/images/53865367b0acbe6f6e2c836df71e0a28.tar.gz success in 1.72016611s
ctr -n k8s.io images import - load image /var/lib/images/53ff218a0bd3594f734251deb2c55861.tar.gz success in 2.107559498s
ctr -n k8s.io images import - load image /var/lib/images/57d2d387f426b14ba99b22e5c3d27dea.tar.gz success in 1.422554184s
ctr -n k8s.io images import - load image /var/lib/images/5946e29adf88edf45a84fe027412d120.tar.gz success in 1.602880105s
ctr -n k8s.io images import - load image /var/lib/images/5bc00166154b6d8a403bb594df270cab.tar.gz success in 1.557697044s
ctr -n k8s.io images import - load image /var/lib/images/5c449066c446b6a0bbd593c59000fd8f.tar.gz success in 1.321493182s
ctr -n k8s.io images import - load image /var/lib/images/5d03bc9d2b45784d705950e5303b8cec.tar.gz success in 1.905423099s
ctr -n k8s.io images import - load image /var/lib/images/60e3259aec388d63121ae8f4c4fbd19c.tar.gz success in 908.405654ms
ctr -n k8s.io images import - load image /var/lib/images/659782fc6c37b1d2836331770aabb79f.tar.gz success in 4.708214239s
ctr -n k8s.io images import - load image /var/lib/images/65e281311204b733f9f7a2670e55c7cd.tar.gz success in 1.919080768s
ctr -n k8s.io images import - load image /var/lib/images/6854e6f16027ae4614fb845541ecc17a.tar.gz success in 609.116846ms
ctr -n k8s.io images import - load image /var/lib/images/6934dd38e4bf92123c99cb94056e6ffa.tar.gz success in 753.145657ms
ctr -n k8s.io images import - load image /var/lib/images/6aa207a19c98a526c5a561365de35007.tar.gz success in 1.266403428s
ctr -n k8s.io images import - load image /var/lib/images/6b489cec4ff643a3e3374d2fc9aef8ce.tar.gz success in 665.83045ms
ctr -n k8s.io images import - load image /var/lib/images/6dc3d4a6d4151e29fafdd5791f701635.tar.gz success in 11.701013204s
ctr -n k8s.io images import - load image /var/lib/images/70d12223ffa48890c97c6ab345444ea1.tar.gz success in 2.595609117s
ctr -n k8s.io images import - load image /var/lib/images/764b141bd301b011fe1363a2f4d63421.tar.gz success in 2.602689269s
ctr -n k8s.io images import - load image /var/lib/images/798ec8617eee19bd43d164964fc6a0dc.tar.gz success in 979.34698ms
ctr -n k8s.io images import - load image /var/lib/images/82b3c50ec8294f00203c41e76382968d.tar.gz success in 8.326490635s
ctr -n k8s.io images import - load image /var/lib/images/835ef35c7b4c81fd6e4b351843176003.tar.gz success in 1.515357699s
ctr -n k8s.io images import - load image /var/lib/images/8524ee56a089c201f455e4a7a3a4149b.tar.gz success in 1.488149595s
ctr -n k8s.io images import - load image /var/lib/images/8602b601c51eb5a80eab678d6c1bc2c9.tar.gz success in 1.307001667s
ctr -n k8s.io images import - load image /var/lib/images/8c40851650797584748986b79202573d.tar.gz success in 2.918964443s
ctr -n k8s.io images import - load image /var/lib/images/8e0b7fb72271f2726452b636d096dad3.tar.gz success in 863.782457ms
ctr -n k8s.io images import - load image /var/lib/images/8e6b64c02f0d4faa20d56544f4d047d8.tar.gz success in 782.805878ms
ctr -n k8s.io images import - load image /var/lib/images/8f8914eff8744896102bcabc62ce7153.tar.gz success in 909.609678ms
ctr -n k8s.io images import - load image /var/lib/images/8f9365015fa9729b3c32061d5afdf3ca.tar.gz success in 1.037676816s
ctr -n k8s.io images import - load image /var/lib/images/8fd7a550f75fbcdc656abb4bdc6318e4.tar.gz success in 4.083733561s
ctr -n k8s.io images import - load image /var/lib/images/9105d3aa3e3bc985e63fe59b19aa0aa1.tar.gz success in 1.621463118s
ctr -n k8s.io images import - load image /var/lib/images/9479f84355fb475a5e93bcf3735e42ea.tar.gz success in 988.858039ms
ctr -n k8s.io images import - load image /var/lib/images/94db882d91892c25da99a82942c328fe.tar.gz success in 3.922333998s
ctr -n k8s.io images import - load image /var/lib/images/98a5793552dd2072e7cdefd95d01d6db.tar.gz success in 1.948603065s
ctr -n k8s.io images import - load image /var/lib/images/9996310041baf0dac998145aa1f01102.tar.gz success in 1.233812234s
ctr -n k8s.io images import - load image /var/lib/images/99e8a6c4806a5a516698a230a1eb4891.tar.gz success in 2.520237386s
ctr -n k8s.io images import - load image /var/lib/images/9e4595017cd045b80dc10f204d44e8c4.tar.gz success in 409.115123ms
ctr -n k8s.io images import - load image /var/lib/images/9ed47fe2d60f9e5cf350c0986b42e4d0.tar.gz success in 1.339817487s
ctr -n k8s.io images import - load image /var/lib/images/a019d387e343c10aec18235b432c7da9.tar.gz success in 1.744455459s
ctr -n k8s.io images import - load image /var/lib/images/a08c71b99ada4b7fa8f2f1588713e4cf.tar.gz success in 2.847925108s
ctr -n k8s.io images import - load image /var/lib/images/a3183bac5b360fdd806f1f75996d6edb.tar.gz success in 3.656986434s
ctr -n k8s.io images import - load image /var/lib/images/a360b8590a286dfcf071f68b9acd42fe.tar.gz success in 499.035359ms
ctr -n k8s.io images import - load image /var/lib/images/a57f9c9f8b17948d22ee4ec4fd2484a8.tar.gz success in 1.362869611s
ctr -n k8s.io images import - load image /var/lib/images/a5da98a23e40df02b7ce11285c6055eb.tar.gz success in 1.411528444s
ctr -n k8s.io images import - load image /var/lib/images/b0075ee207100f2192ba1af491b83ec8.tar.gz success in 2.561494742s
ctr -n k8s.io images import - load image /var/lib/images/b5fce673a9cd32f4d07cd27f5d8e4fbd.tar.gz success in 1.998049842s
ctr -n k8s.io images import - load image /var/lib/images/b6e132653e9f596fe84ad7b973e26fc1.tar.gz success in 2.27428324s
ctr -n k8s.io images import - load image /var/lib/images/b88d1e388cc14fe1058d3eef98f51777.tar.gz success in 831.316471ms
ctr -n k8s.io images import - load image /var/lib/images/bce1d20e0a6066e4a5ae3934b5c1b3ff.tar.gz success in 2.188148809s
ctr -n k8s.io images import - load image /var/lib/images/bcf814f4338c76e0b938c4b25d5f8082.tar.gz success in 6.723441614s
ctr -n k8s.io images import - load image /var/lib/images/bf42733453c596b0ce267ac921fa50db.tar.gz success in 1.366400869s
ctr -n k8s.io images import - load image /var/lib/images/c1d319b4975f072076d2f6d2773cca23.tar.gz success in 1.137656995s
ctr -n k8s.io images import - load image /var/lib/images/c246bcba43e89fa0666f21bf5fe04eea.tar.gz success in 1.995466496s
ctr -n k8s.io images import - load image /var/lib/images/c2fbd194e0d1880c5b416d0754cb227d.tar.gz success in 6.795233698s
ctr -n k8s.io images import - load image /var/lib/images/c4d1fedb4c10944b1111a6c3f9ef8049.tar.gz success in 2.191697583s
ctr -n k8s.io images import - load image /var/lib/images/c59223aedb4c9d94490852fd2258f8ba.tar.gz success in 992.796068ms
ctr -n k8s.io images import - load image /var/lib/images/cbcbbca9fd86e935bb9d1307b4673dbd.tar.gz success in 19.103200821s
ctr -n k8s.io images import - load image /var/lib/images/ccd78a80cc2b52277af85b6bcc9455e9.tar.gz success in 1.752675918s
ctr -n k8s.io images import - load image /var/lib/images/cd8a3a9a3db143a512f52fbe82ba90ca.tar.gz success in 2.474548432s
ctr -n k8s.io images import - load image /var/lib/images/d33969534e1d39f49f6763f4e01b8ae1.tar.gz success in 299.720709ms
ctr -n k8s.io images import - load image /var/lib/images/dab8945b4157b27869d528599632435a.tar.gz success in 1.235749277s
ctr -n k8s.io images import - load image /var/lib/images/dc1903709b3cc9a8953e07d77c2ef810.tar.gz success in 307.296473ms
ctr -n k8s.io images import - load image /var/lib/images/e1f970ba6092e5e73f9a186a3af2948a.tar.gz success in 1.429480255s
ctr -n k8s.io images import - load image /var/lib/images/e314251bf2d09351d2c82eca70494580.tar.gz success in 1.791249817s
ctr -n k8s.io images import - load image /var/lib/images/e3488e92ca154482fcbfd3074c6676dd.tar.gz success in 632.972735ms
ctr -n k8s.io images import - load image /var/lib/images/e39c544370a86db27b057a6c39426924.tar.gz success in 1.558635176s
ctr -n k8s.io images import - load image /var/lib/images/e679d3642b8d793ed975f1ce02b85729.tar.gz success in 702.387809ms
ctr -n k8s.io images import - load image /var/lib/images/e9dbb9c13ae82910f9cf5c134333b60d.tar.gz success in 972.909698ms
ctr -n k8s.io images import - load image /var/lib/images/ea3a84040dda28aac479931349367c23.tar.gz success in 15.734262944s
ctr -n k8s.io images import - load image /var/lib/images/eb3107527ecfa5e264f53d9338572d99.tar.gz success in 1.663668811s
ctr -n k8s.io images import - load image /var/lib/images/edb3700ce0da8a0f61407ef16579c7e0.tar.gz success in 3.430681414s
ctr -n k8s.io images import - load image /var/lib/images/f1c5a9ed0343fd797219c2ecb931448a.tar.gz success in 1.515944517s
ctr -n k8s.io images import - load image /var/lib/images/f1f760cfce1072474f1e5ff586f081e9.tar.gz success in 653.207734ms
ctr -n k8s.io images import - load image /var/lib/images/f244deb177abefe8cfe75a510e3b1487.tar.gz success in 362.970964ms
ctr -n k8s.io images import - load image /var/lib/images/f2f49b5d7b7b9094287cbef0c2abdf40.tar.gz success in 1.309802774s
ctr -n k8s.io images import - load image /var/lib/images/f577ed4ce643ec98259f0bdee8cec451.tar.gz success in 1.201339994s
ctr -n k8s.io images import - load image /var/lib/images/f5ad7b3d68cbf95b0790a1027ddd274c.tar.gz success in 1.475056448s
ctr -n k8s.io images import - load image /var/lib/images/f753daa50eb5945d07231fd04507442f.tar.gz success in 1.52974578s
ctr -n k8s.io images import - load image /var/lib/images/fc3bb5fa9363aadb10eef7dbed8db0fd.tar.gz success in 2.081352789s
ctr -n k8s.io images import - load image /var/lib/images/fcbd5d74b008eaef5960ef26e949f0ed.tar.gz success in 2.560171081s
ctr -n k8s.io images import - load image /var/lib/images/fd38c4d756b51f32ba1f882970876219.tar.gz success in 6.845782497s
ctr -n k8s.io images import - load image /var/lib/images/fdd574a0ffe035565b58f28c2d1f7728.tar.gz success in 7.802992119s
ctr -n k8s.io images import - load image /var/lib/images/ks-installer.tar.gz success in 4.837325878s
2024-07-09T02:10:44.915-0700	[A] zhaoyu: PreloadImagesService success (4m35.174023608s)
2024-07-09T02:10:44.916-0700	[A] Remote: CopyKubeConfig
2024-07-09T02:10:44.955-0700	[A] zhaoyu: CopyKubeConfig success (39.890065ms)
2024-07-09T02:10:44.955-0700	[A] zhaoyu: AddMasterTaint(k3s) skipped (21.142µs)
2024-07-09T02:10:44.955-0700	[A] Remote: AddWorkerLabel
AddWorkerLabel successed: node/zhaoyu labeled
2024-07-09T02:10:45.231-0700	[A] zhaoyu: AddWorkerLabel success (275.43526ms)
2024-07-09T02:10:45.231-0700	[Module] StatusModule
2024-07-09T02:10:45.231-0700	[A] Remote: GetClusterStatus(k3s)
2024-07-09T02:10:45.240-0700	check remote file exist: true
2024-07-09T02:10:45.246-0700	[exec] zhaoyu CMD: k3s --version | grep 'k3s' | awk '{print $3}', OUTPUT: 
v1.22.16+k3s1
2024-07-09T02:10:45.261-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "cat /var/lib/rancher/k3s/server/node-token", OUTPUT: 
K109127db9982d010b5d98b7c0e0cd0dcb3bbe45c1e577568dc679effa9709c11ab::server:810bb7bc29b04638b4a6a61bdd133a34
2024-07-09T02:10:45.404-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses", OUTPUT: 
zhaoyu   v1.22.16+k3s1   [map[address:192.168.50.187 type:InternalIP] map[address:zhaoyu type:Hostname]]
2024-07-09T02:10:45.404-0700	[A] zhaoyu: GetClusterStatus(k3s) success (173.312445ms)
2024-07-09T02:10:45.404-0700	[Module] K3sJoinNodesModule
2024-07-09T02:10:45.404-0700	[A] zhaoyu: GenerateK3sService skipped (17.043µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: GenerateK3sServiceEnv skipped (3.505µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: GenerateK3sRegistryConfig skipped (3.814µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: EnableK3sService skipped (3.039µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: CopyKubeConfig skipped (3.54µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: SyncKubeConfigToWorker(k3s) skipped (3.824µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: AddMasterTaint(k3s) skipped (5.602µs)
2024-07-09T02:10:45.404-0700	[A] zhaoyu: AddWorkerLabel skipped (3.953µs)
2024-07-09T02:10:45.404-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.404-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.404-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.404-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:45.405-0700	[Module] DeployNetworkPluginModule
2024-07-09T02:10:45.405-0700	[A] Remote: GenerateCalico
2024-07-09T02:10:45.425-0700	scp local file /project/pkg/zhaoyu/network-plugin.yaml to remote /tmp/kubekey/etc/kubernetes/network-plugin.yaml success
2024-07-09T02:10:45.447-0700	[A] zhaoyu: GenerateCalico success (42.224717ms)
2024-07-09T02:10:45.447-0700	[A] Remote: DeployCalico
2024-07-09T02:10:46.154-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl apply -f /etc/kubernetes/network-plugin.yaml --force", OUTPUT: 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
2024-07-09T02:10:46.154-0700	[A] zhaoyu: DeployCalico success (707.616933ms)
2024-07-09T02:10:46.155-0700	[Module] ConfigureKubernetesModule
2024-07-09T02:10:46.155-0700	[A] Remote: ConfigureKubernetes
2024-07-09T02:10:46.155-0700	[A] zhaoyu: ConfigureKubernetes success (38.735µs)
2024-07-09T02:10:46.155-0700	[Module] ChownModule
2024-07-09T02:10:46.155-0700	[A] Remote: ChownFileAndDir
2024-07-09T02:10:46.163-0700	check remote file exist: true
2024-07-09T02:10:46.176-0700	[A] zhaoyu: ChownFileAndDir success (21.795243ms)
2024-07-09T02:10:46.176-0700	[Module] AutoRenewCertsModule
2024-07-09T02:10:46.176-0700	[A] Remote: GenerateK8sCertsRenewScript
2024-07-09T02:10:46.194-0700	scp local file /project/pkg/zhaoyu/k8s-certs-renew.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/k8s-certs-renew.sh success
2024-07-09T02:10:46.221-0700	[A] zhaoyu: GenerateK8sCertsRenewScript success (44.63462ms)
2024-07-09T02:10:46.221-0700	[A] Remote: GenerateK8sCertsRenewService
2024-07-09T02:10:46.239-0700	scp local file /project/pkg/zhaoyu/k8s-certs-renew.service to remote /tmp/kubekey/etc/systemd/system/k8s-certs-renew.service success
2024-07-09T02:10:46.261-0700	[A] zhaoyu: GenerateK8sCertsRenewService success (40.078189ms)
2024-07-09T02:10:46.261-0700	[A] Remote: GenerateK8sCertsRenewTimer
2024-07-09T02:10:46.279-0700	scp local file /project/pkg/zhaoyu/k8s-certs-renew.timer to remote /tmp/kubekey/etc/systemd/system/k8s-certs-renew.timer success
2024-07-09T02:10:46.301-0700	[A] zhaoyu: GenerateK8sCertsRenewTimer success (39.370992ms)
2024-07-09T02:10:46.301-0700	[A] Remote: EnableK8sCertsRenewService
2024-07-09T02:10:46.501-0700	[A] zhaoyu: EnableK8sCertsRenewService success (200.654893ms)
2024-07-09T02:10:46.501-0700	[Module] SaveKubeConfigModule
2024-07-09T02:10:46.501-0700	[A] LocalHost: SaveKubeConfig(k3s)
2024-07-09T02:10:46.532-0700	[A] LocalHost: SaveKubeConfig(k3s) success (30.280676ms)
2024-07-09T02:10:46.532-0700	[Module] AddonsModule
2024-07-09T02:10:46.532-0700	[A] LocalHost: InstallAddons
2024-07-09T02:10:46.532-0700	[A] LocalHost: InstallAddons success (32.575µs)
2024-07-09T02:10:46.532-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:46.532-0700	pauseTag: 3.5, corednsTag: 1.8.0
2024-07-09T02:10:46.532-0700	[Module] DeployStorageClassModule
2024-07-09T02:10:46.676-0700	[A] Remote: GenerateOpenEBSManifest
2024-07-09T02:10:46.697-0700	scp local file /project/pkg/zhaoyu/local-volume.yaml to remote /tmp/kubekey/etc/kubernetes/addons/local-volume.yaml success
2024-07-09T02:10:46.720-0700	[A] zhaoyu: GenerateOpenEBSManifest success (188.653354ms)
2024-07-09T02:10:46.861-0700	[A] Remote: DeployOpenEBS
2024-07-09T02:10:47.146-0700	[A] zhaoyu: DeployOpenEBS success (425.879478ms)


##########################################################################################
#
#  这里开始涉及到 ks-installer
#  这里的 AddKsInstallerConfig、CreateKubeSphereNamespace、SetupKsInstallerConfig... 是并行的
#
##########################################################################################
2024-07-09T02:10:47.146-0700	[Module] DeployKubeSphereModule
2024-07-09T02:10:47.146-0700	[A] Remote: GenerateKsInstallerCRD
2024-07-09T02:10:47.168-0700	scp local file /project/pkg/zhaoyu/kubesphere.yaml to remote /tmp/kubekey/etc/kubernetes/addons/kubesphere.yaml success
2024-07-09T02:10:47.193-0700	[A] zhaoyu: GenerateKsInstallerCRD success (46.574709ms)
2024-07-09T02:10:47.193-0700	[A] Remote: ApplyKsInstaller
2024-07-09T02:10:47.773-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl apply -f /etc/kubernetes/addons/kubesphere.yaml --force", OUTPUT: 
namespace/kubesphere-system created
serviceaccount/ks-installer created
customresourcedefinition.apiextensions.k8s.io/clusterconfigurations.installer.kubesphere.io created
clusterrole.rbac.authorization.k8s.io/ks-installer created
clusterrolebinding.rbac.authorization.k8s.io/ks-installer created
deployment.apps/ks-installer created     ---------->>>>>>>>>>>>>>>>>>>>>>>>>>> 看这里，这里是 deployment ks-installer
2024-07-09T02:10:47.773-0700	[A] zhaoyu: ApplyKsInstaller success (580.215611ms)
2024-07-09T02:10:47.773-0700	[A] Remote: AddKsInstallerConfig    -------->>>>> 这里其实是在NewKubeRunner 时就已经提取了模版信息，ks v3.3.0 的数据；文件位于 ./pkg/version/kubesphere/templates/cc_v330.go；就是 kind：ClusterConfiguration，它内部有很多注释的地方，估计是在 ks-installer 中去做了替换。
2024-07-09T02:10:47.782-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "echo 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 | base64 -d >> /etc/kubernetes/addons/kubesphere.yaml", OUTPUT: 

2024-07-09T02:10:47.783-0700	[A] zhaoyu: AddKsInstallerConfig success (9.336321ms)
2024-07-09T02:10:47.783-0700	[A] Remote: CreateKubeSphereNamespace
2024-07-09T02:10:47.992-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "cat <<EOF | /usr/local/bin/kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: kubesphere-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: kubesphere-monitoring-system
EOF
", OUTPUT: 
namespace/kubesphere-system unchanged
namespace/kubesphere-monitoring-system created
2024-07-09T02:10:47.992-0700	[A] zhaoyu: CreateKubeSphereNamespace success (209.241564ms)

##############################
#  SetupKsInstallerConfig 这个东西是干嘛的？做生成了 etcd 相关的信息，然后创建 secret？
##############################
2024-07-09T02:10:47.992-0700	[A] Remote: SetupKsInstallerConfig   ------>>>>> 这里会修改 /etc/kubernetes/addons/kubesphere.yaml，包括会修改 etcd 的地址
2024-07-09T02:10:48.129-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs --from-file=etcd-client-ca.crt=/etc/ssl/etcd/ssl/ca.pem --from-file=etcd-client.crt=/etc/ssl/etcd/ssl/node-zhaoyu.pem --from-file=etcd-client.key=/etc/ssl/etcd/ssl/node-zhaoyu-key.pem", OUTPUT: 
secret/kube-etcd-client-certs created
2024-07-09T02:10:48.188-0700	[A] zhaoyu: SetupKsInstallerConfig success (195.909852ms)

##############################
# 所有这次执行的才应该是最完整的？至少包括了 etcd 的地址吧。
##############################
2024-07-09T02:10:48.188-0700	[A] Remote: ApplyKsInstaller   ------------>>>>>> 这里是第二次执行；为什么会执行两次呢？？？？？
2024-07-09T02:10:50.925-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl apply -f /etc/kubernetes/addons/kubesphere.yaml --force", OUTPUT: 
namespace/kubesphere-system unchanged
serviceaccount/ks-installer unchanged
customresourcedefinition.apiextensions.k8s.io/clusterconfigurations.installer.kubesphere.io unchanged
clusterrole.rbac.authorization.k8s.io/ks-installer unchanged
clusterrolebinding.rbac.authorization.k8s.io/ks-installer unchanged
deployment.apps/ks-installer unchanged
clusterconfiguration.installer.kubesphere.io/ks-installer created   ----->>>>>>>>>>>>>>>>>> 这个是新建的
2024-07-09T02:10:50.925-0700	[A] zhaoyu: ApplyKsInstaller success (2.737048708s)



2024-07-09T02:10:50.925-0700	[Module] CheckResultModule
2024-07-09T02:10:50.925-0700	[A] Remote: CheckKsInstallerResult
2024-07-09T02:10:56.662-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
start to sync ks-installer's log2024-07-09T02:10:56-07:00 INFO     : shell-operator latest
2024-07-09T02:10:56-07:00 INFO     : Use temporary dir: /tmp/shell-operator
2024-07-09T02:10:56-07:00 INFO     : Initialize hooks manager ...
2024-07-09T02:10:56-07:00 INFO     : Search and load hooks ...
2024-07-09T02:10:56-07:00 INFO     : HTTP SERVER Listening on 0.0.0.0:9115
2024-07-09T02:10:56-07:00 INFO     : Load hook config from '/hooks/kubesphere/installRunner.py'
2024-07-09T02:10:56-07:00 INFO     : Load hook config from '/hooks/kubesphere/schedule.sh'
2024-07-09T02:10:56-07:00 INFO     : Initializing schedule manager ...
2024-07-09T02:10:56-07:00 INFO     : KUBE Init Kubernetes client
2024-07-09T02:10:56-07:00 INFO     : KUBE-INIT Kubernetes client is configured successfully
2024-07-09T02:10:56-07:00 INFO     : MAIN: run main loop
2024-07-09T02:10:56-07:00 INFO     : MAIN: add onStartup tasks
2024-07-09T02:10:56-07:00 INFO     : QUEUE add all HookRun@OnStartup
2024-07-09T02:10:56-07:00 INFO     : Running schedule manager ...
2024-07-09T02:10:56-07:00 INFO     : MSTOR Create new metric shell_operator_live_ticks
2024-07-09T02:10:56-07:00 INFO     : MSTOR Create new metric shell_operator_tasks_queue_length
2024-07-09T02:10:56-07:00 INFO     : GVR for kind 'ClusterConfiguration' is installer.kubesphere.io/v1alpha1, Resource=clusterconfigurations
2024-07-09T02:10:56-07:00 INFO     : EVENT Kube event '98ee2388-3f31-4af5-b54a-9bede1e65a67'
2024-07-09T02:10:56-07:00 INFO     : QUEUE add TASK_HOOK_RUN@KUBE_EVENTS kubesphere/installRunner.py
2024-07-09T02:10:59-07:00 INFO     : TASK_RUN HookRun@KUBE_EVENTS kubesphere/installRunner.py
2024-07-09T02:10:59-07:00 INFO     : Running hook 'kubesphere/installRunner.py' binding 'KUBE_EVENTS' ...
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [preinstall : KubeSphere | Stopping if Kubernetes version is nonsupport] ***
ok: [localhost] => {
    "changed": false,
    "msg": "All assertions passed"
}

TASK [preinstall : KubeSphere | Checking StorageClass] *************************
changed: [localhost]

TASK [preinstall : KubeSphere | Stopping if StorageClass was not found] ********
skipping: [localhost]

TASK [preinstall : KubeSphere | Checking default StorageClass] *****************
changed: [localhost]

TASK [preinstall : KubeSphere | Stopping if default StorageClass was not found] ***
ok: [localhost] => {
    "changed": false,
    "msg": "All assertions passed"
}

TASK [preinstall : KubeSphere | Checking KubeSphere component] *****************
changed: [localhost]

TASK [preinstall : KubeSphere | Getting KubeSphere component version] **********
skipping: [localhost] => (item=ks-openldap) 
skipping: [localhost] => (item=ks-redis) 
skipping: [localhost] => (item=ks-minio) 
skipping: [localhost] => (item=ks-openpitrix) 
skipping: [localhost] => (item=elasticsearch-logging) 
skipping: [localhost] => (item=elasticsearch-logging-curator) 
skipping: [localhost] => (item=istio) 
skipping: [localhost] => (item=istio-init) 
skipping: [localhost] => (item=jaeger-operator) 
skipping: [localhost] => (item=ks-jenkins) 
skipping: [localhost] => (item=ks-sonarqube) 
skipping: [localhost] => (item=logging-fluentbit-operator) 
skipping: [localhost] => (item=uc) 
skipping: [localhost] => (item=metrics-server) 

PLAY RECAP *********************************************************************
localhost                  : ok=6    changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
2024-07-09T02:11:02.020-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [Metrics-Server | Getting metrics-server installation files] **************
skipping: [localhost]

TASK [metrics-server : Metrics-Server | Creating manifests] ********************
skipping: [localhost] => (item={'file': 'metrics-server.yaml'}) 

TASK [metrics-server : Metrics-Server | Checking Metrics-Server] ***************
skipping: [localhost]

TASK [Metrics-Server | Uninstalling old metrics-server] ************************
skipping: [localhost]

TASK [Metrics-Server | Installing new metrics-server] **************************
skipping: [localhost]

TASK [metrics-server : Metrics-Server | Waitting for metrics.k8s.io ready] *****
skipping: [localhost]

TASK [Metrics-Server | Importing metrics-server status] ************************
skipping: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=10   rescued=0    ignored=0   


-------->>>>>>>>>>>>>>> 新 playbook：common
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [common : KubeSphere | Checking kube-node-lease namespace] ****************    -->>>>>>> 这里开始进入 common
changed: [localhost]

TASK [common : KubeSphere | Getting system namespaces] *************************
ok: [localhost]

TASK [common : set_fact] *******************************************************
ok: [localhost]

TASK [common : debug] **********************************************************
ok: [localhost] => {
    "msg": [
        "kubesphere-system",
        "kubesphere-controls-system",
        "kubesphere-monitoring-system",
        "kubesphere-monitoring-federated",
        "kube-node-lease"
    ]
}

TASK [common : KubeSphere | Creating KubeSphere namespace] *********************
changed: [localhost] => (item=kubesphere-system)
changed: [localhost] => (item=kubesphere-controls-system)
changed: [localhost] => (item=kubesphere-monitoring-system)
changed: [localhost] => (item=kubesphere-monitoring-federated)
changed: [localhost] => (item=kube-node-lease)

TASK [common : KubeSphere | Labeling system-workspace] *************************
changed: [localhost] => (item=default)
changed: [localhost] => (item=kube-public)
changed: [localhost] => (item=kube-system)
changed: [localhost] => (item=kubesphere-system)
changed: [localhost] => (item=kubesphere-controls-system)
changed: [localhost] => (item=kubesphere-monitoring-system)
changed: [localhost] => (item=kubesphere-monitoring-federated)
changed: [localhost] => (item=kube-node-lease)

TASK [common : KubeSphere | Labeling namespace for network policy] *************
changed: [localhost]

TASK [common : KubeSphere | Getting Kubernetes master num] *********************
changed: [localhost]

TASK [common : KubeSphere | Setting master num] ********************************
ok: [localhost]

TASK [KubeSphere | Getting common component installation files] ****************
changed: [localhost] => (item=common)

TASK [common : KubeSphere | Checking Kubernetes version] ***********************
changed: [localhost]

TASK [KubeSphere | Getting common component installation files] ****************
2024-07-09T02:11:07.372-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=snapshot-controller)

TASK [common : KubeSphere | Creating snapshot controller values] ***************
changed: [localhost] => (item={'name': 'custom-values-snapshot-controller', 'file': 'custom-values-snapshot-controller.yaml'})

TASK [common : KubeSphere | Updating snapshot crd] *****************************
changed: [localhost]

TASK [common : KubeSphere | Deploying snapshot controller] *********************
changed: [localhost]

TASK [KubeSphere | Creating common component manifests] ************************
changed: [localhost] => (item={'path': 'redis', 'file': 'redis.yaml'})

TASK [common : KubeSphere | Generet Random password] ***************************
ok: [localhost]

TASK [common : KubeSphere | Creating Redis Password Secret] ********************
changed: [localhost]

TASK [common : KubeSphere | Getting redis installation files] ******************
skipping: [localhost] => (item=redis-ha) 

TASK [common : KubeSphere | Creating manifests] ********************************
skipping: [localhost] => (item={'name': 'custom-values-redis', 'file': 'custom-values-redis.yaml'}) 

TASK [common : KubeSphere | Checking old redis status] *************************
skipping: [localhost]

TASK [common : KubeSphere | Deleting and backup old redis svc] *****************
skipping: [localhost]

TASK [common : KubeSphere | Deploying redis] ***********************************
skipping: [localhost]

TASK [common : KubeSphere | Deploying redis] ***********************************
changed: [localhost] => (item=redis.yaml)

TASK [common : KubeSphere | Importing redis status] ****************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=21   changed=15   unreachable=0    failed=0    skipped=8    rescued=0    ignored=0   
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [ks-core/init-token : KubeSphere | Creating KubeSphere directory] *********
ok: [localhost]

TASK [ks-core/init-token : KubeSphere | Getting installation init files] *******
2024-07-09T02:11:12.730-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=jwt-script)

TASK [ks-core/init-token : KubeSphere | Creating KubeSphere Secret] ************
changed: [localhost]

TASK [ks-core/init-token : KubeSphere | Creating KubeSphere Secret] ************
ok: [localhost]

TASK [ks-core/init-token : KubeSphere | Creating KubeSphere Secret] ************
skipping: [localhost]

TASK [ks-core/init-token : KubeSphere | Enabling Token Script] *****************
changed: [localhost]

TASK [ks-core/init-token : KubeSphere | Getting KubeSphere Token] **************
changed: [localhost]

TASK [ks-core/init-token : KubeSphere | Checking KubeSphere secrets] ***********
changed: [localhost]

TASK [ks-core/init-token : KubeSphere | Deleting KubeSphere secret] ************
skipping: [localhost]

TASK [ks-core/init-token : KubeSphere | Creating components token] *************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Setting Kubernetes version] ***************
ok: [localhost]

TASK [ks-core/ks-core : KubeSphere | Getting Kubernetes master num] ************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Setting master num] ***********************
ok: [localhost]

TASK [ks-core/ks-core : KubeSphere | Override master num] **********************
skipping: [localhost]

TASK [ks-core/ks-core : KubeSphere | Setting enableHA] *************************
ok: [localhost]

TASK [ks-core/ks-core : KubeSphere | Checking ks-core Helm Release] ************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Checking ks-core Exsit] *******************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Convert ks-core to helm mananged] *********
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'serviceaccounts', 'resource': 'kubesphere-cluster-admin', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'serviceaccounts', 'resource': 'kubesphere-router-serviceaccount', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'role', 'resource': 'system:kubesphere-router-role', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'rolebinding', 'resource': 'nginx-ingress-role-nisa-binding', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'deployment', 'resource': 'default-http-backend', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-controls-system', 'kind': 'service', 'resource': 'default-http-backend', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'secrets', 'resource': 'ks-controller-manager-webhook-cert', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'serviceaccounts', 'resource': 'kubesphere', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'configmaps', 'resource': 'ks-router-config', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'configmaps', 'resource': 'sample-bookinfo', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'clusterroles', 'resource': 'system:kubesphere-router-clusterrole', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'clusterrolebindings', 'resource': 'system:nginx-ingress-clusterrole-nisa-binding', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'clusterrolebindings', 'resource': 'system:kubesphere-cluster-admin', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'clusterrolebindings', 'resource': 'kubesphere', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'services', 'resource': 'ks-apiserver', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'services', 'resource': 'ks-controller-manager', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'deployments', 'resource': 'ks-apiserver', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'deployments', 'resource': 'ks-controller-manager', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'validatingwebhookconfigurations', 'resource': 'users.iam.kubesphere.io', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'validatingwebhookconfigurations', 'resource': 'resourcesquotas.quota.kubesphere.io', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'validatingwebhookconfigurations', 'resource': 'network.kubesphere.io', 'release': 'ks-core'}) 
skipping: [localhost] => (item={'ns': 'kubesphere-system', 'kind': 'users.iam.kubesphere.io', 'resource': 'admin', 'release': 'ks-core'}) 

TASK [ks-core/ks-core : KubeSphere | Patch admin user] *************************
skipping: [localhost]

TASK [ks-core/ks-core : KubeSphere | Getting ks-core helm charts] **************
2024-07-09T02:11:18.073-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:11:23.407-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=ks-core-config)
changed: [localhost] => (item=ks-core)

TASK [ks-core/ks-core : KubeSphere | Creating manifests] ***********************
changed: [localhost] => (item={'path': 'ks-core', 'file': 'custom-values-ks-core.yaml'})

TASK [ks-core/ks-core : KubeSphere | Upgrade CRDs] *****************************
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/tenant.kubesphere.io_workspaces.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/storage.kubesphere.io_storageclasseraccessor.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/network.kubesphere.io_ipamhandles.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_federatedrolebindings.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/application.kubesphere.io_helmreleases.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/servicemesh.kubesphere.io_servicepolicies.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/cluster.kubesphere.io_clusters.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/network.kubesphere.io_ippools.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_rolebases.yaml)
2024-07-09T02:11:28.770-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_loginrecords.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/network.kubesphere.io_ipamblocks.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/gateway.kubesphere.io_nginxes.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/gateway.kubesphere.io_gateways.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_workspaceroles.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/network.kubesphere.io_namespacenetworkpolicies.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_federatedroles.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/tenant.kubesphere.io_workspacetemplates.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/application.kubesphere.io_helmapplications.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/servicemesh.kubesphere.io_strategies.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/quota.kubesphere.io_resourcequotas.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_groups.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/application.kubesphere.io_helmrepos.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/app_v1beta1_application.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_workspacerolebindings.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_globalrolebindings.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_users.yaml)
2024-07-09T02:11:34.109-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/application.kubesphere.io_helmapplicationversions.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_groupbindings.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_globalroles.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/application.kubesphere.io_helmcategories.yaml)
changed: [localhost] => (item=/kubesphere/kubesphere/ks-core-config/crds/iam.kubesphere.io_federatedusers.yaml)

TASK [ks-core/ks-core : KubeSphere | Creating ks-core-config] ******************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Creating ks-core] *************************
changed: [localhost]

TASK [ks-core/ks-core : KubeSphere | Importing ks-core status] *****************
changed: [localhost]

TASK [ks-core/prepare : KubeSphere | Checking core components (1)] *************
changed: [localhost]

TASK [ks-core/prepare : KubeSphere | Checking core components (2)] *************
changed: [localhost]

TASK [ks-core/prepare : KubeSphere | Checking core components (3)] *************
skipping: [localhost]

TASK [ks-core/prepare : KubeSphere | Checking core components (4)] *************
skipping: [localhost]

TASK [ks-core/prepare : KubeSphere | Updating ks-core status] ******************
skipping: [localhost]

TASK [ks-core/prepare : set_fact] **********************************************
skipping: [localhost]

TASK [ks-core/prepare : KubeSphere | Creating KubeSphere directory] ************
ok: [localhost]

TASK [ks-core/prepare : KubeSphere | Getting installation init files] **********
2024-07-09T02:11:39.463-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost] => (item=ks-init)

TASK [ks-core/prepare : KubeSphere | Initing KubeSphere] ***********************
changed: [localhost] => (item=role-templates.yaml)

TASK [ks-core/prepare : KubeSphere | Generating kubeconfig-admin] **************
skipping: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=26   changed=19   unreachable=0    failed=0    skipped=13   rescued=0    ignored=0   
Start installing monitoring
Start installing multicluster
Start installing openpitrix
Start installing network
**************************************************
Waiting for all tasks to be completed ...
task network status is successful  (1/4)
task openpitrix status is successful  (2/4)
task multicluster status is successful  (3/4)
2024-07-09T02:11:44.799-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:11:50.144-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:11:55.496-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:00.814-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:06.211-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:11.550-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:16.918-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:22.284-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
2024-07-09T02:12:27.652-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
task monitoring status is successful  (4/4)
**************************************************
Collecting installation results ...
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [ks-core/config : KubeSphere | Getting sonarqube host] ********************
changed: [localhost]

TASK [ks-core/config : KubeSphere | Getting sonarqube token] *******************
changed: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : KubeSphere | Getting es index prefix] *******************
changed: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : KubeSphere | Getting token] *****************************
2024-07-09T02:12:33.012-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1
changed: [localhost]

TASK [ks-core/config : KubeSphere | Getting ks-secret] *************************
changed: [localhost]

TASK [ks-core/config : debug] **************************************************
ok: [localhost] => {
    "msg": "Current Kubernetes version is v1.22.16+k3s1"
}

TASK [ks-core/config : KubeSphere | Setting kubectl image version] *************
ok: [localhost] => (item={'key': 'v1.22.0', 'value': 'v1.22.0'})
skipping: [localhost] => (item={'key': 'v1.21.0', 'value': 'v1.21.0'}) 
skipping: [localhost] => (item={'key': 'v1.20.0', 'value': 'v1.20.0'}) 
skipping: [localhost] => (item={'key': 'v1.19.0', 'value': 'v1.19.1'}) 
skipping: [localhost] => (item={'key': 'v1.18.0', 'value': 'v1.18.0'}) 
skipping: [localhost] => (item={'key': 'v1.17.0', 'value': 'v1.17.0'}) 
skipping: [localhost] => (item={'key': 'v1.16.0', 'value': 'v1.16.0'}) 
skipping: [localhost] => (item={'key': 'v1.15.0', 'value': 'v1.0.0'}) 

TASK [ks-core/config : debug] **************************************************
ok: [localhost] => {
    "msg": "Current kubectl image version is v1.22.0"
}

TASK [ks-core/config : KubeSphere | Getting Kubernetes master num] *************
changed: [localhost]

TASK [ks-core/config : KubeSphere | Setting master num] ************************
ok: [localhost]

TASK [ks-core/config : KubeSphere | Creating manifests] ************************
changed: [localhost] => (item={'name': 'kubesphere-config', 'file': 'kubesphere-config.yaml', 'type': 'cm'})

TASK [ks-core/config : KubeSphere | Initing KubeSphere] ************************
changed: [localhost] => (item=kubesphere-config.yaml)

TASK [ks-core/config : OpenPitrix | Getting openpitrix jobs installation files] ***
skipping: [localhost] => (item=openpitrix) 

TASK [ks-core/config : OpenPitrix | Creating openpitrix jobs manifests] ********
skipping: [localhost] => (item={'path': 'openpitrix', 'file': 'ks-openpitrix-upgrade.yaml'}) 

TASK [ks-core/config : OpenPitrix | Upgrade OpenPitrix] ************************
skipping: [localhost]

TASK [ks-core/config : OpenPitrix | Delete namespace openpitrix-system] ********
skipping: [localhost]

TASK [ks-core/config : Alerting | Checking status v3.0.0] **********************
changed: [localhost]

TASK [ks-core/config : Alerting | Getting migration files] *********************
skipping: [localhost] => (item=alerting-migration) 

TASK [ks-core/config : Alerting | Getting migration files] *********************
skipping: [localhost] => (item={'path': 'alerting-migration', 'file': 'ks-alerting-migration.yaml'}) 

TASK [ks-core/config : Alerting | Creating migration job] **********************
skipping: [localhost]

TASK [ks-core/config : Alerting | Getting migration job status] ****************
skipping: [localhost]

TASK [ks-core/config : Alerting | Getting migration job status] ****************
skipping: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=14   changed=9    unreachable=0    failed=0    skipped=15   rescued=0    ignored=0   
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that
the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************

TASK [download : Generating images list] ***************************************
skipping: [localhost]

TASK [download : Synchronizing images] *****************************************

TASK [kubesphere-defaults : KubeSphere | Setting images' namespace override] ***
skipping: [localhost]

TASK [kubesphere-defaults : KubeSphere | Configuring defaults] *****************
ok: [localhost] => {
    "msg": "Check roles/kubesphere-defaults/defaults/main.yml"
}

TASK [ks-core/config : KubeSphere | Getting sonarqube host] ********************
changed: [localhost]

TASK [ks-core/config : KubeSphere | Getting sonarqube token] *******************
changed: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : KubeSphere | Getting es index prefix] *******************
changed: [localhost]

TASK [ks-core/config : set_fact] ***********************************************
skipping: [localhost]

TASK [ks-core/config : KubeSphere | Getting token] *****************************
changed: [localhost]

TASK [ks-core/config : KubeSphere | Getting ks-secret] *************************
changed: [localhost]

TASK [ks-core/config : debug] **************************************************
ok: [localhost] => {
    "msg": "Current Kubernetes version is v1.22.16+k3s1"
}

TASK [ks-core/config : KubeSphere | Setting kubectl image version] *************
ok: [localhost] => (item={'key': 'v1.22.0', 'value': 'v1.22.0'})
skipping: [localhost] => (item={'key': 'v1.21.0', 'value': 'v1.21.0'}) 
skipping: [localhost] => (item={'key': 'v1.20.0', 'value': 'v1.20.0'}) 
skipping: [localhost] => (item={'key': 'v1.19.0', 'value': 'v1.19.1'}) 
skipping: [localhost] => (item={'key': 'v1.18.0', 'value': 'v1.18.0'}) 
skipping: [localhost] => (item={'key': 'v1.17.0', 'value': 'v1.17.0'}) 
skipping: [localhost] => (item={'key': 'v1.16.0', 'value': 'v1.16.0'}) 
skipping: [localhost] => (item={'key': 'v1.15.0', 'value': 'v1.0.0'}) 

TASK [ks-core/config : debug] **************************************************
ok: [localhost] => {
    "msg": "Current kubectl image version is v1.22.0"
}

TASK [ks-core/config : KubeSphere | Getting Kubernetes master num] *************
changed: [localhost]

TASK [ks-core/config : KubeSphere | Setting master num] ************************
ok: [localhost]

TASK [ks-core/config : KubeSphere | Creating manifests] ************************
ok: [localhost] => (item={'name': 'kubesphere-config', 'file': 'kubesphere-config.yaml', 'type': 'cm'})

TASK [ks-core/config : KubeSphere | Initing KubeSphere] ************************
changed: [localhost] => (item=kubesphere-config.yaml)

TASK [ks-core/config : OpenPitrix | Getting openpitrix jobs installation files] ***
skipping: [localhost] => (item=openpitrix) 

TASK [ks-core/config : OpenPitrix | Creating openpitrix jobs manifests] ********
skipping: [localhost] => (item={'path': 'openpitrix', 'file': 'ks-openpitrix-upgrade.yaml'}) 

TASK [ks-core/config : OpenPitrix | Upgrade OpenPitrix] ************************
skipping: [localhost]
2024-07-09T02:12:38.412-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running" 
ls: /kubesphere/playbooks/kubesphere_running: No such file or directory
command terminated with exit code 1: Process exited with status 1

TASK [ks-core/config : OpenPitrix | Delete namespace openpitrix-system] ********
skipping: [localhost]

TASK [ks-core/config : Alerting | Checking status v3.0.0] **********************
changed: [localhost]

TASK [ks-core/config : Alerting | Getting migration files] *********************
skipping: [localhost] => (item=alerting-migration) 

TASK [ks-core/config : Alerting | Getting migration files] *********************
skipping: [localhost] => (item={'path': 'alerting-migration', 'file': 'ks-alerting-migration.yaml'}) 

TASK [ks-core/config : Alerting | Creating migration job] **********************
skipping: [localhost]

TASK [ks-core/config : Alerting | Getting migration job status] ****************
skipping: [localhost]

TASK [ks-core/config : Alerting | Getting migration job status] ****************
skipping: [localhost]

TASK [check-result : ks-devops | Getting ks-sonarqube NodeIp] ******************
changed: [localhost]

TASK [check-result : KubeSphere | Waiting for ks-apiserver] ********************
changed: [localhost]

TASK [check-result : KubeSphere | Importing ks-core status] ********************
changed: [localhost]

TASK [check-result : KubeSphere | Creating info_file] **************************
changed: [localhost -> localhost] => (item={'name': 'welcome', 'file': 'kubesphere_running', 'type': 'info'})

PLAY RECAP *********************************************************************
localhost                  : ok=18   changed=12   unreachable=0    failed=0    skipped=15   rescued=0    ignored=0   
#####################################################
###              Welcome to KubeSphere!           ###
#####################################################

NOTES：
  1. After you log into the console, please check the
     monitoring status of service components in
     "Cluster Management". If any service is not
     ready, please wait patiently until all components 
     are up and running.
  2. Please change the default password after login.

#####################################################
https://kubesphere.io             2024-07-09 02:12:39
#####################################################

2024-07-09T02:12:44.116-0700	[A] zhaoyu: CheckKsInstallerResult success (1m53.190939764s)
2024-07-09T02:12:44.116-0700	close connection zhaoyu
2024-07-09T02:12:44.116-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -f > /tmp/.ks-installer.log", ERROR: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -f > /tmp/.ks-installer.log" 
: wait: remote command exited without exit status or exit signal
2024-07-09T02:12:44.116-0700	[Job] Install Terminus execute successfully!!! (7m2.471522471s)
Installation is complete.
	
	Please check the result using the command:
	
		kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l 'app in (ks-install, ks-installer)' -o jsonpath='{.items[0].metadata.name}') -f   
	
	tail log:  , [Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -f > /tmp/.ks-installer.log" 
: wait: remote command exited without exit status or exit signal]2024-07-09T02:12:49.120-0700	[exec] zhaoyu CMD: sudo -E /bin/bash -c "/usr/local/bin/kubectl exec -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-installer -o jsonpath='{.items[0].metadata.name}') -- ls /kubesphere/playbooks/kubesphere_running", ERROR: failed to get SSH session: connection closed
